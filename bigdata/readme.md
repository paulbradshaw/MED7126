# Working with big data

This tutorial takes you through the process of working with a big data set (over 10 million rows) using the following tools:

* Command line (Terminal on a Mac/Linux or PowerShell on PC)
* Google Cloud Storage
* Google BigQuery
* SQL

## Part 1: Creating and understanding a big dataset using command line

Although many 'big data' scenarios don't involve you actually creating the dataset yourself, in this tutorial we are going to compile a very large dataset ourselves, using command line, just because we can.

You can find out more about command line in [my GitHub repo on that topic](https://github.com/paulbradshaw/commandline), but here are the key points as far as big data is concerned:

* Command line allows you to work with files on your computer without having to open software packages like Excel or Word
* One thing we can do with command line, for example, is combine hundreds of spreadsheet files into a very large dataset
* We can also use command line to find out the first line in a large dataset, or to count how many rows it has - which we couldn't do in Excel

In this part you'll learn how.

### Getting the data - in pieces

We'll be working with police crime data

### Combining data using `cat` or `type`

To combine multiple CSV files you can use the `cat` command (Mac/Linux) or `type` (Windows). You'll [find instructions here](https://github.com/paulbradshaw/commandline/blob/master/joining.md)

## Part 2: Uploading the data to Google Cloud Storage

## Part 3: Adding the data to a project in Google BigQuery

## Part 4: Querying the data using SQL

### Export the results as a CSV
